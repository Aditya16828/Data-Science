{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9720f2c",
   "metadata": {},
   "source": [
    ">Removing punctuations (@, # , ! , etc)\n",
    "\n",
    ">Removing url\n",
    "\n",
    ">Removing stopwords\n",
    "\n",
    ">Lowercasing\n",
    "\n",
    ">Tokenization\n",
    "\n",
    ">Stemming\n",
    "\n",
    ">Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b5f19d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:03.168157Z",
     "start_time": "2023-03-25T08:57:02.131172Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baf1ad4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:03.187481Z",
     "start_time": "2023-03-25T08:57:03.170400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"spam.csv\", encoding=\"ISO-8859-1\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "856277fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:03.190877Z",
     "start_time": "2023-03-25T08:57:03.188556Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5g/9xpg7d6d4114s98tv10y9rgm0000gn/T/ipykernel_15138/497376691.py:3: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('max_colwidth', -1)\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2661b41c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:03.195597Z",
     "start_time": "2023-03-25T08:57:03.192054Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_column', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_seq_items', None)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "pd.set_option('expand_frame_repr', True)\n",
    "pd.options.display.max_colwidth\n",
    "pd.options.display.max_info_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "855d2129",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:03.202327Z",
     "start_time": "2023-03-25T08:57:03.197100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                                                                            v2  \\\n",
       "0                                              Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...   \n",
       "1                                                                                                                                Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's   \n",
       "3                                                                                                            U dun say so early hor... U c already then say...   \n",
       "4                                                                                                Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "  Unnamed: 2 Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN        NaN  \n",
       "1        NaN        NaN        NaN  \n",
       "2        NaN        NaN        NaN  \n",
       "3        NaN        NaN        NaN  \n",
       "4        NaN        NaN        NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d0bd771",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:03.209831Z",
     "start_time": "2023-03-25T08:57:03.204136Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                                                                            v2  \n",
       "0                                              Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...  \n",
       "1                                                                                                                                Ok lar... Joking wif u oni...  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's  \n",
       "3                                                                                                            U dun say so early hor... U c already then say...  \n",
       "4                                                                                                Nah I don't think he goes to usf, he lives around here though  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['v1','v2']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bca92b83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:03.213976Z",
     "start_time": "2023-03-25T08:57:03.210936Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c44d7154",
   "metadata": {},
   "source": [
    "### Removing punctuations (@, # , ! , etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e5cf176",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:03.216699Z",
     "start_time": "2023-03-25T08:57:03.215000Z"
    }
   },
   "outputs": [],
   "source": [
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d366620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:03.220073Z",
     "start_time": "2023-03-25T08:57:03.217921Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65aa199d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:03.223199Z",
     "start_time": "2023-03-25T08:57:03.221082Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    str = \"\".join([i for i in text if i not in punctuation])\n",
    "    return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab4b6c1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:03.259790Z",
     "start_time": "2023-03-25T08:57:03.227120Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "      <td>Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                                                                            v2  \\\n",
       "0                                              Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...   \n",
       "1                                                                                                                                Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's   \n",
       "3                                                                                                            U dun say so early hor... U c already then say...   \n",
       "4                                                                                                Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "                                                                                                                                                   clean  \n",
       "0                                                 Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat  \n",
       "1                                                                                                                                Ok lar Joking wif u oni  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s  \n",
       "3                                                                                                            U dun say so early hor U c already then say  \n",
       "4                                                                                            Nah I dont think he goes to usf he lives around here though  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean'] = df['v2'].apply(lambda x : remove_punct(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41c4e18b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:03.264228Z",
     "start_time": "2023-03-25T08:57:03.260890Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0c334d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:03.270578Z",
     "start_time": "2023-03-25T08:57:03.266282Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c127ebf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:03.274322Z",
     "start_time": "2023-03-25T08:57:03.271713Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39cc1148",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:03.278173Z",
     "start_time": "2023-03-25T08:57:03.275556Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f07b8560",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:03.284118Z",
     "start_time": "2023-03-25T08:57:03.279352Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "      <td>Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                                                                            v2  \\\n",
       "0                                              Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...   \n",
       "1                                                                                                                                Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's   \n",
       "3                                                                                                            U dun say so early hor... U c already then say...   \n",
       "4                                                                                                Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "                                                                                                                                                   clean  \n",
       "0                                                 Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat  \n",
       "1                                                                                                                                Ok lar Joking wif u oni  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s  \n",
       "3                                                                                                            U dun say so early hor U c already then say  \n",
       "4                                                                                            Nah I dont think he goes to usf he lives around here though  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "453c540f",
   "metadata": {},
   "source": [
    "### Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0412f04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:03.292102Z",
     "start_time": "2023-03-25T08:57:03.285292Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>clean</th>\n",
       "      <th>clean_lower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "      <td>Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat</td>\n",
       "      <td>go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>nah i dont think he goes to usf he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                                                                            v2  \\\n",
       "0                                              Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...   \n",
       "1                                                                                                                                Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's   \n",
       "3                                                                                                            U dun say so early hor... U c already then say...   \n",
       "4                                                                                                Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "                                                                                                                                                   clean  \\\n",
       "0                                                 Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat   \n",
       "1                                                                                                                                Ok lar Joking wif u oni   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s   \n",
       "3                                                                                                            U dun say so early hor U c already then say   \n",
       "4                                                                                            Nah I dont think he goes to usf he lives around here though   \n",
       "\n",
       "                                                                                                                                             clean_lower  \n",
       "0                                                 go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat  \n",
       "1                                                                                                                                ok lar joking wif u oni  \n",
       "2  free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s  \n",
       "3                                                                                                            u dun say so early hor u c already then say  \n",
       "4                                                                                            nah i dont think he goes to usf he lives around here though  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_lower'] = df['clean'].apply(lambda x : x.lower())\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5abe4a5",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8da0f81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:03.296056Z",
     "start_time": "2023-03-25T08:57:03.293359Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'given', 'text', 'data', 'We', 'can', 'choose', 'any', 'method', 'based', 'on', 'langauge', 'library', 'and', 'purpose', 'of', 'modeling']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "tx = \"\"\"There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling.\"\"\"\n",
    "tokens1 = re.findall(\"[\\w]+\", tx)\n",
    "print(tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b27b5b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a80a0b8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:03.299123Z",
     "start_time": "2023-03-25T08:57:03.297159Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    tok = re.findall(\"[\\w]+\", text)\n",
    "    return tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f1f0911",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:03.354750Z",
     "start_time": "2023-03-25T08:57:03.300109Z"
    }
   },
   "outputs": [],
   "source": [
    "df['clean_lower_tokenized'] = df['clean_lower'].apply(lambda x : tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bad679e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:03.362132Z",
     "start_time": "2023-03-25T08:57:03.355928Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>clean</th>\n",
       "      <th>clean_lower</th>\n",
       "      <th>clean_lower_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "      <td>Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat</td>\n",
       "      <td>go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, cine, there, got, amore, wat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to, 87121, to, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, then, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>nah i dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                                                                            v2  \\\n",
       "0                                              Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...   \n",
       "1                                                                                                                                Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's   \n",
       "3                                                                                                            U dun say so early hor... U c already then say...   \n",
       "4                                                                                                Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "                                                                                                                                                   clean  \\\n",
       "0                                                 Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat   \n",
       "1                                                                                                                                Ok lar Joking wif u oni   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s   \n",
       "3                                                                                                            U dun say so early hor U c already then say   \n",
       "4                                                                                            Nah I dont think he goes to usf he lives around here though   \n",
       "\n",
       "                                                                                                                                             clean_lower  \\\n",
       "0                                                 go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat   \n",
       "1                                                                                                                                ok lar joking wif u oni   \n",
       "2  free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s   \n",
       "3                                                                                                            u dun say so early hor u c already then say   \n",
       "4                                                                                            nah i dont think he goes to usf he lives around here though   \n",
       "\n",
       "                                                                                                                                                                clean_lower_tokenized  \n",
       "0                                                         [go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, cine, there, got, amore, wat]  \n",
       "1                                                                                                                                                      [ok, lar, joking, wif, u, oni]  \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to, 87121, to, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]  \n",
       "3                                                                                                                             [u, dun, say, so, early, hor, u, c, already, then, say]  \n",
       "4                                                                                                           [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03d0a21d",
   "metadata": {},
   "source": [
    "### Stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49ba4e73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:03.366189Z",
     "start_time": "2023-03-25T08:57:03.363400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'URGENT! Your Mobile No. was awarded å£2000 Bonus Caller Prize on 5/9/03 This is our final try to contact U! Call from Landline 09064019788 BOX42WR29C, 150PPM'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[120,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e511f20c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:04.115211Z",
     "start_time": "2023-03-25T08:57:03.367293Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c171aee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:04.124306Z",
     "start_time": "2023-03-25T08:57:04.116273Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10405"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75507a63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:04.139502Z",
     "start_time": "2023-03-25T08:57:04.125397Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['إذ',\n",
       " 'إذا',\n",
       " 'إذما',\n",
       " 'إذن',\n",
       " 'أف',\n",
       " 'أقل',\n",
       " 'أكثر',\n",
       " 'ألا',\n",
       " 'إلا',\n",
       " 'التي',\n",
       " 'الذي',\n",
       " 'الذين',\n",
       " 'اللاتي',\n",
       " 'اللائي',\n",
       " 'اللتان',\n",
       " 'اللتيا',\n",
       " 'اللتين',\n",
       " 'اللذان',\n",
       " 'اللذين',\n",
       " 'اللواتي',\n",
       " 'إلى',\n",
       " 'إليك',\n",
       " 'إليكم',\n",
       " 'إليكما',\n",
       " 'إليكن',\n",
       " 'أم',\n",
       " 'أما',\n",
       " 'أما',\n",
       " 'إما',\n",
       " 'أن',\n",
       " 'إن',\n",
       " 'إنا',\n",
       " 'أنا',\n",
       " 'أنت',\n",
       " 'أنتم',\n",
       " 'أنتما',\n",
       " 'أنتن',\n",
       " 'إنما',\n",
       " 'إنه',\n",
       " 'أنى',\n",
       " 'أنى',\n",
       " 'آه',\n",
       " 'آها',\n",
       " 'أو',\n",
       " 'أولاء',\n",
       " 'أولئك',\n",
       " 'أوه',\n",
       " 'آي',\n",
       " 'أي',\n",
       " 'أيها',\n",
       " 'إي',\n",
       " 'أين',\n",
       " 'أين',\n",
       " 'أينما',\n",
       " 'إيه',\n",
       " 'بخ',\n",
       " 'بس',\n",
       " 'بعد',\n",
       " 'بعض',\n",
       " 'بك',\n",
       " 'بكم',\n",
       " 'بكم',\n",
       " 'بكما',\n",
       " 'بكن',\n",
       " 'بل',\n",
       " 'بلى',\n",
       " 'بما',\n",
       " 'بماذا',\n",
       " 'بمن',\n",
       " 'بنا',\n",
       " 'به',\n",
       " 'بها',\n",
       " 'بهم',\n",
       " 'بهما',\n",
       " 'بهن',\n",
       " 'بي',\n",
       " 'بين',\n",
       " 'بيد',\n",
       " 'تلك',\n",
       " 'تلكم',\n",
       " 'تلكما',\n",
       " 'ته',\n",
       " 'تي',\n",
       " 'تين',\n",
       " 'تينك',\n",
       " 'ثم',\n",
       " 'ثمة',\n",
       " 'حاشا',\n",
       " 'حبذا',\n",
       " 'حتى',\n",
       " 'حيث',\n",
       " 'حيثما',\n",
       " 'حين',\n",
       " 'خلا',\n",
       " 'دون',\n",
       " 'ذا',\n",
       " 'ذات',\n",
       " 'ذاك',\n",
       " 'ذان',\n",
       " 'ذانك',\n",
       " 'ذلك',\n",
       " 'ذلكم',\n",
       " 'ذلكما',\n",
       " 'ذلكن',\n",
       " 'ذه',\n",
       " 'ذو',\n",
       " 'ذوا',\n",
       " 'ذواتا',\n",
       " 'ذواتي',\n",
       " 'ذي',\n",
       " 'ذين',\n",
       " 'ذينك',\n",
       " 'ريث',\n",
       " 'سوف',\n",
       " 'سوى',\n",
       " 'شتان',\n",
       " 'عدا',\n",
       " 'عسى',\n",
       " 'عل',\n",
       " 'على',\n",
       " 'عليك',\n",
       " 'عليه',\n",
       " 'عما',\n",
       " 'عن',\n",
       " 'عند',\n",
       " 'غير',\n",
       " 'فإذا',\n",
       " 'فإن',\n",
       " 'فلا',\n",
       " 'فمن',\n",
       " 'في',\n",
       " 'فيم',\n",
       " 'فيما',\n",
       " 'فيه',\n",
       " 'فيها',\n",
       " 'قد',\n",
       " 'كأن',\n",
       " 'كأنما',\n",
       " 'كأي',\n",
       " 'كأين',\n",
       " 'كذا',\n",
       " 'كذلك',\n",
       " 'كل',\n",
       " 'كلا',\n",
       " 'كلاهما',\n",
       " 'كلتا',\n",
       " 'كلما',\n",
       " 'كليكما',\n",
       " 'كليهما',\n",
       " 'كم',\n",
       " 'كم',\n",
       " 'كما',\n",
       " 'كي',\n",
       " 'كيت',\n",
       " 'كيف',\n",
       " 'كيفما',\n",
       " 'لا',\n",
       " 'لاسيما',\n",
       " 'لدى',\n",
       " 'لست',\n",
       " 'لستم',\n",
       " 'لستما',\n",
       " 'لستن',\n",
       " 'لسن',\n",
       " 'لسنا',\n",
       " 'لعل',\n",
       " 'لك',\n",
       " 'لكم',\n",
       " 'لكما',\n",
       " 'لكن',\n",
       " 'لكنما',\n",
       " 'لكي',\n",
       " 'لكيلا',\n",
       " 'لم',\n",
       " 'لما',\n",
       " 'لن',\n",
       " 'لنا',\n",
       " 'له',\n",
       " 'لها',\n",
       " 'لهم',\n",
       " 'لهما',\n",
       " 'لهن',\n",
       " 'لو',\n",
       " 'لولا',\n",
       " 'لوما',\n",
       " 'لي',\n",
       " 'لئن',\n",
       " 'ليت',\n",
       " 'ليس',\n",
       " 'ليسا',\n",
       " 'ليست',\n",
       " 'ليستا',\n",
       " 'ليسوا',\n",
       " 'ما',\n",
       " 'ماذا',\n",
       " 'متى',\n",
       " 'مذ',\n",
       " 'مع',\n",
       " 'مما',\n",
       " 'ممن',\n",
       " 'من',\n",
       " 'منه',\n",
       " 'منها',\n",
       " 'منذ',\n",
       " 'مه',\n",
       " 'مهما',\n",
       " 'نحن',\n",
       " 'نحو',\n",
       " 'نعم',\n",
       " 'ها',\n",
       " 'هاتان',\n",
       " 'هاته',\n",
       " 'هاتي',\n",
       " 'هاتين',\n",
       " 'هاك',\n",
       " 'هاهنا',\n",
       " 'هذا',\n",
       " 'هذان',\n",
       " 'هذه',\n",
       " 'هذي',\n",
       " 'هذين',\n",
       " 'هكذا',\n",
       " 'هل',\n",
       " 'هلا',\n",
       " 'هم',\n",
       " 'هما',\n",
       " 'هن',\n",
       " 'هنا',\n",
       " 'هناك',\n",
       " 'هنالك',\n",
       " 'هو',\n",
       " 'هؤلاء',\n",
       " 'هي',\n",
       " 'هيا',\n",
       " 'هيت',\n",
       " 'هيهات',\n",
       " 'والذي',\n",
       " 'والذين',\n",
       " 'وإذ',\n",
       " 'وإذا',\n",
       " 'وإن',\n",
       " 'ولا',\n",
       " 'ولكن',\n",
       " 'ولو',\n",
       " 'وما',\n",
       " 'ومن',\n",
       " 'وهو',\n",
       " 'يا',\n",
       " 'أبٌ',\n",
       " 'أخٌ',\n",
       " 'حمٌ',\n",
       " 'فو',\n",
       " 'أنتِ',\n",
       " 'يناير',\n",
       " 'فبراير',\n",
       " 'مارس',\n",
       " 'أبريل',\n",
       " 'مايو',\n",
       " 'يونيو',\n",
       " 'يوليو',\n",
       " 'أغسطس',\n",
       " 'سبتمبر',\n",
       " 'أكتوبر',\n",
       " 'نوفمبر',\n",
       " 'ديسمبر',\n",
       " 'جانفي',\n",
       " 'فيفري',\n",
       " 'مارس',\n",
       " 'أفريل',\n",
       " 'ماي',\n",
       " 'جوان',\n",
       " 'جويلية',\n",
       " 'أوت',\n",
       " 'كانون',\n",
       " 'شباط',\n",
       " 'آذار',\n",
       " 'نيسان',\n",
       " 'أيار',\n",
       " 'حزيران',\n",
       " 'تموز',\n",
       " 'آب',\n",
       " 'أيلول',\n",
       " 'تشرين',\n",
       " 'دولار',\n",
       " 'دينار',\n",
       " 'ريال',\n",
       " 'درهم',\n",
       " 'ليرة',\n",
       " 'جنيه',\n",
       " 'قرش',\n",
       " 'مليم',\n",
       " 'فلس',\n",
       " 'هللة',\n",
       " 'سنتيم',\n",
       " 'يورو',\n",
       " 'ين',\n",
       " 'يوان',\n",
       " 'شيكل',\n",
       " 'واحد',\n",
       " 'اثنان',\n",
       " 'ثلاثة',\n",
       " 'أربعة',\n",
       " 'خمسة',\n",
       " 'ستة',\n",
       " 'سبعة',\n",
       " 'ثمانية',\n",
       " 'تسعة',\n",
       " 'عشرة',\n",
       " 'أحد',\n",
       " 'اثنا',\n",
       " 'اثني',\n",
       " 'إحدى',\n",
       " 'ثلاث',\n",
       " 'أربع',\n",
       " 'خمس',\n",
       " 'ست',\n",
       " 'سبع',\n",
       " 'ثماني',\n",
       " 'تسع',\n",
       " 'عشر',\n",
       " 'ثمان',\n",
       " 'سبت',\n",
       " 'أحد',\n",
       " 'اثنين',\n",
       " 'ثلاثاء',\n",
       " 'أربعاء',\n",
       " 'خميس',\n",
       " 'جمعة',\n",
       " 'أول',\n",
       " 'ثان',\n",
       " 'ثاني',\n",
       " 'ثالث',\n",
       " 'رابع',\n",
       " 'خامس',\n",
       " 'سادس',\n",
       " 'سابع',\n",
       " 'ثامن',\n",
       " 'تاسع',\n",
       " 'عاشر',\n",
       " 'حادي',\n",
       " 'أ',\n",
       " 'ب',\n",
       " 'ت',\n",
       " 'ث',\n",
       " 'ج',\n",
       " 'ح',\n",
       " 'خ',\n",
       " 'د',\n",
       " 'ذ',\n",
       " 'ر',\n",
       " 'ز',\n",
       " 'س',\n",
       " 'ش',\n",
       " 'ص',\n",
       " 'ض',\n",
       " 'ط',\n",
       " 'ظ',\n",
       " 'ع',\n",
       " 'غ',\n",
       " 'ف',\n",
       " 'ق',\n",
       " 'ك',\n",
       " 'ل',\n",
       " 'م',\n",
       " 'ن',\n",
       " 'ه',\n",
       " 'و',\n",
       " 'ي',\n",
       " 'ء',\n",
       " 'ى',\n",
       " 'آ',\n",
       " 'ؤ',\n",
       " 'ئ',\n",
       " 'أ',\n",
       " 'ة',\n",
       " 'ألف',\n",
       " 'باء',\n",
       " 'تاء',\n",
       " 'ثاء',\n",
       " 'جيم',\n",
       " 'حاء',\n",
       " 'خاء',\n",
       " 'دال',\n",
       " 'ذال',\n",
       " 'راء',\n",
       " 'زاي',\n",
       " 'سين',\n",
       " 'شين',\n",
       " 'صاد',\n",
       " 'ضاد',\n",
       " 'طاء',\n",
       " 'ظاء',\n",
       " 'عين',\n",
       " 'غين',\n",
       " 'فاء',\n",
       " 'قاف',\n",
       " 'كاف',\n",
       " 'لام',\n",
       " 'ميم',\n",
       " 'نون',\n",
       " 'هاء',\n",
       " 'واو',\n",
       " 'ياء',\n",
       " 'همزة',\n",
       " 'ي',\n",
       " 'نا',\n",
       " 'ك',\n",
       " 'كن',\n",
       " 'ه',\n",
       " 'إياه',\n",
       " 'إياها',\n",
       " 'إياهما',\n",
       " 'إياهم',\n",
       " 'إياهن',\n",
       " 'إياك',\n",
       " 'إياكما',\n",
       " 'إياكم',\n",
       " 'إياك',\n",
       " 'إياكن',\n",
       " 'إياي',\n",
       " 'إيانا',\n",
       " 'أولالك',\n",
       " 'تانِ',\n",
       " 'تانِك',\n",
       " 'تِه',\n",
       " 'تِي',\n",
       " 'تَيْنِ',\n",
       " 'ثمّ',\n",
       " 'ثمّة',\n",
       " 'ذانِ',\n",
       " 'ذِه',\n",
       " 'ذِي',\n",
       " 'ذَيْنِ',\n",
       " 'هَؤلاء',\n",
       " 'هَاتانِ',\n",
       " 'هَاتِه',\n",
       " 'هَاتِي',\n",
       " 'هَاتَيْنِ',\n",
       " 'هَذا',\n",
       " 'هَذانِ',\n",
       " 'هَذِه',\n",
       " 'هَذِي',\n",
       " 'هَذَيْنِ',\n",
       " 'الألى',\n",
       " 'الألاء',\n",
       " 'أل',\n",
       " 'أنّى',\n",
       " 'أيّ',\n",
       " 'ّأيّان',\n",
       " 'أنّى',\n",
       " 'أيّ',\n",
       " 'ّأيّان',\n",
       " 'ذيت',\n",
       " 'كأيّ',\n",
       " 'كأيّن',\n",
       " 'بضع',\n",
       " 'فلان',\n",
       " 'وا',\n",
       " 'آمينَ',\n",
       " 'آهِ',\n",
       " 'آهٍ',\n",
       " 'آهاً',\n",
       " 'أُفٍّ',\n",
       " 'أُفٍّ',\n",
       " 'أفٍّ',\n",
       " 'أمامك',\n",
       " 'أمامكَ',\n",
       " 'أوّهْ',\n",
       " 'إلَيْكَ',\n",
       " 'إلَيْكَ',\n",
       " 'إليكَ',\n",
       " 'إليكنّ',\n",
       " 'إيهٍ',\n",
       " 'بخٍ',\n",
       " 'بسّ',\n",
       " 'بَسْ',\n",
       " 'بطآن',\n",
       " 'بَلْهَ',\n",
       " 'حاي',\n",
       " 'حَذارِ',\n",
       " 'حيَّ',\n",
       " 'حيَّ',\n",
       " 'دونك',\n",
       " 'رويدك',\n",
       " 'سرعان',\n",
       " 'شتانَ',\n",
       " 'شَتَّانَ',\n",
       " 'صهْ',\n",
       " 'صهٍ',\n",
       " 'طاق',\n",
       " 'طَق',\n",
       " 'عَدَسْ',\n",
       " 'كِخ',\n",
       " 'مكانَك',\n",
       " 'مكانَك',\n",
       " 'مكانَك',\n",
       " 'مكانكم',\n",
       " 'مكانكما',\n",
       " 'مكانكنّ',\n",
       " 'نَخْ',\n",
       " 'هاكَ',\n",
       " 'هَجْ',\n",
       " 'هلم',\n",
       " 'هيّا',\n",
       " 'هَيْهات',\n",
       " 'وا',\n",
       " 'واهاً',\n",
       " 'وراءَك',\n",
       " 'وُشْكَانَ',\n",
       " 'وَيْ',\n",
       " 'يفعلان',\n",
       " 'تفعلان',\n",
       " 'يفعلون',\n",
       " 'تفعلون',\n",
       " 'تفعلين',\n",
       " 'اتخذ',\n",
       " 'ألفى',\n",
       " 'تخذ',\n",
       " 'ترك',\n",
       " 'تعلَّم',\n",
       " 'جعل',\n",
       " 'حجا',\n",
       " 'حبيب',\n",
       " 'خال',\n",
       " 'حسب',\n",
       " 'خال',\n",
       " 'درى',\n",
       " 'رأى',\n",
       " 'زعم',\n",
       " 'صبر',\n",
       " 'ظنَّ',\n",
       " 'عدَّ',\n",
       " 'علم',\n",
       " 'غادر',\n",
       " 'ذهب',\n",
       " 'وجد',\n",
       " 'ورد',\n",
       " 'وهب',\n",
       " 'أسكن',\n",
       " 'أطعم',\n",
       " 'أعطى',\n",
       " 'رزق',\n",
       " 'زود',\n",
       " 'سقى',\n",
       " 'كسا',\n",
       " 'أخبر',\n",
       " 'أرى',\n",
       " 'أعلم',\n",
       " 'أنبأ',\n",
       " 'حدَث',\n",
       " 'خبَّر',\n",
       " 'نبَّا',\n",
       " 'أفعل به',\n",
       " 'ما أفعله',\n",
       " 'بئس',\n",
       " 'ساء',\n",
       " 'طالما',\n",
       " 'قلما',\n",
       " 'لات',\n",
       " 'لكنَّ',\n",
       " 'ءَ',\n",
       " 'أجل',\n",
       " 'إذاً',\n",
       " 'أمّا',\n",
       " 'إمّا',\n",
       " 'إنَّ',\n",
       " 'أنًّ',\n",
       " 'أى',\n",
       " 'إى',\n",
       " 'أيا',\n",
       " 'ب',\n",
       " 'ثمَّ',\n",
       " 'جلل',\n",
       " 'جير',\n",
       " 'رُبَّ',\n",
       " 'س',\n",
       " 'علًّ',\n",
       " 'ف',\n",
       " 'كأنّ',\n",
       " 'كلَّا',\n",
       " 'كى',\n",
       " 'ل',\n",
       " 'لات',\n",
       " 'لعلَّ',\n",
       " 'لكنَّ',\n",
       " 'لكنَّ',\n",
       " 'م',\n",
       " 'نَّ',\n",
       " 'هلّا',\n",
       " 'وا',\n",
       " 'أل',\n",
       " 'إلّا',\n",
       " 'ت',\n",
       " 'ك',\n",
       " 'لمّا',\n",
       " 'ن',\n",
       " 'ه',\n",
       " 'و',\n",
       " 'ا',\n",
       " 'ي',\n",
       " 'تجاه',\n",
       " 'تلقاء',\n",
       " 'جميع',\n",
       " 'حسب',\n",
       " 'سبحان',\n",
       " 'شبه',\n",
       " 'لعمر',\n",
       " 'مثل',\n",
       " 'معاذ',\n",
       " 'أبو',\n",
       " 'أخو',\n",
       " 'حمو',\n",
       " 'فو',\n",
       " 'مئة',\n",
       " 'مئتان',\n",
       " 'ثلاثمئة',\n",
       " 'أربعمئة',\n",
       " 'خمسمئة',\n",
       " 'ستمئة',\n",
       " 'سبعمئة',\n",
       " 'ثمنمئة',\n",
       " 'تسعمئة',\n",
       " 'مائة',\n",
       " 'ثلاثمائة',\n",
       " 'أربعمائة',\n",
       " 'خمسمائة',\n",
       " 'ستمائة',\n",
       " 'سبعمائة',\n",
       " 'ثمانمئة',\n",
       " 'تسعمائة',\n",
       " 'عشرون',\n",
       " 'ثلاثون',\n",
       " 'اربعون',\n",
       " 'خمسون',\n",
       " 'ستون',\n",
       " 'سبعون',\n",
       " 'ثمانون',\n",
       " 'تسعون',\n",
       " 'عشرين',\n",
       " 'ثلاثين',\n",
       " 'اربعين',\n",
       " 'خمسين',\n",
       " 'ستين',\n",
       " 'سبعين',\n",
       " 'ثمانين',\n",
       " 'تسعين',\n",
       " 'بضع',\n",
       " 'نيف',\n",
       " 'أجمع',\n",
       " 'جميع',\n",
       " 'عامة',\n",
       " 'عين',\n",
       " 'نفس',\n",
       " 'لا سيما',\n",
       " 'أصلا',\n",
       " 'أهلا',\n",
       " 'أيضا',\n",
       " 'بؤسا',\n",
       " 'بعدا',\n",
       " 'بغتة',\n",
       " 'تعسا',\n",
       " 'حقا',\n",
       " 'حمدا',\n",
       " 'خلافا',\n",
       " 'خاصة',\n",
       " 'دواليك',\n",
       " 'سحقا',\n",
       " 'سرا',\n",
       " 'سمعا',\n",
       " 'صبرا',\n",
       " 'صدقا',\n",
       " 'صراحة',\n",
       " 'طرا',\n",
       " 'عجبا',\n",
       " 'عيانا',\n",
       " 'غالبا',\n",
       " 'فرادى',\n",
       " 'فضلا',\n",
       " 'قاطبة',\n",
       " 'كثيرا',\n",
       " 'لبيك',\n",
       " 'معاذ',\n",
       " 'أبدا',\n",
       " 'إزاء',\n",
       " 'أصلا',\n",
       " 'الآن',\n",
       " 'أمد',\n",
       " 'أمس',\n",
       " 'آنفا',\n",
       " 'آناء',\n",
       " 'أنّى',\n",
       " 'أول',\n",
       " 'أيّان',\n",
       " 'تارة',\n",
       " 'ثمّ',\n",
       " 'ثمّة',\n",
       " 'حقا',\n",
       " 'صباح',\n",
       " 'مساء',\n",
       " 'ضحوة',\n",
       " 'عوض',\n",
       " 'غدا',\n",
       " 'غداة',\n",
       " 'قطّ',\n",
       " 'كلّما',\n",
       " 'لدن',\n",
       " 'لمّا',\n",
       " 'مرّة',\n",
       " 'قبل',\n",
       " 'خلف',\n",
       " 'أمام',\n",
       " 'فوق',\n",
       " 'تحت',\n",
       " 'يمين',\n",
       " 'شمال',\n",
       " 'ارتدّ',\n",
       " 'استحال',\n",
       " 'أصبح',\n",
       " 'أضحى',\n",
       " 'آض',\n",
       " 'أمسى',\n",
       " 'انقلب',\n",
       " 'بات',\n",
       " 'تبدّل',\n",
       " 'تحوّل',\n",
       " 'حار',\n",
       " 'رجع',\n",
       " 'راح',\n",
       " 'صار',\n",
       " 'ظلّ',\n",
       " 'عاد',\n",
       " 'غدا',\n",
       " 'كان',\n",
       " 'ما انفك',\n",
       " 'ما برح',\n",
       " 'مادام',\n",
       " 'مازال',\n",
       " 'مافتئ',\n",
       " 'ابتدأ',\n",
       " 'أخذ',\n",
       " 'اخلولق',\n",
       " 'أقبل',\n",
       " 'انبرى',\n",
       " 'أنشأ',\n",
       " 'أوشك',\n",
       " 'جعل',\n",
       " 'حرى',\n",
       " 'شرع',\n",
       " 'طفق',\n",
       " 'علق',\n",
       " 'قام',\n",
       " 'كرب',\n",
       " 'كاد',\n",
       " 'هبّa',\n",
       " 'ad',\n",
       " 'altı',\n",
       " 'altmış',\n",
       " 'amma',\n",
       " 'arasında',\n",
       " 'artıq',\n",
       " 'ay',\n",
       " 'az',\n",
       " 'bax',\n",
       " 'belə',\n",
       " 'bəli',\n",
       " 'bəlkə',\n",
       " 'beş',\n",
       " 'bəy',\n",
       " 'bəzən',\n",
       " 'bəzi',\n",
       " 'bilər',\n",
       " 'bir',\n",
       " 'biraz',\n",
       " 'biri',\n",
       " 'birşey',\n",
       " 'biz',\n",
       " 'bizim',\n",
       " 'bizlər',\n",
       " 'bu',\n",
       " 'buna',\n",
       " 'bundan',\n",
       " 'bunların',\n",
       " 'bunu',\n",
       " 'bunun',\n",
       " 'buradan',\n",
       " 'bütün',\n",
       " 'ci',\n",
       " 'cı',\n",
       " 'çox',\n",
       " 'cu',\n",
       " 'cü',\n",
       " 'çünki',\n",
       " 'da',\n",
       " 'daha',\n",
       " 'də',\n",
       " 'dedi',\n",
       " 'dək',\n",
       " 'dən',\n",
       " 'dəqiqə',\n",
       " 'deyil',\n",
       " 'dir',\n",
       " 'doqquz',\n",
       " 'doqsan',\n",
       " 'dörd',\n",
       " 'düz',\n",
       " 'ə',\n",
       " 'edən',\n",
       " 'edir',\n",
       " 'əgər',\n",
       " 'əlbəttə',\n",
       " 'elə',\n",
       " 'əlli',\n",
       " 'ən',\n",
       " 'əslində',\n",
       " 'et',\n",
       " 'etdi',\n",
       " 'etmə',\n",
       " 'etmək',\n",
       " 'faiz',\n",
       " 'gilə',\n",
       " 'görə',\n",
       " 'ha',\n",
       " 'haqqında',\n",
       " 'harada',\n",
       " 'hə',\n",
       " 'heç',\n",
       " 'həm',\n",
       " 'həmin',\n",
       " 'həmişə',\n",
       " 'hər',\n",
       " 'ı',\n",
       " 'idi',\n",
       " 'iki',\n",
       " 'il',\n",
       " 'ildə',\n",
       " 'ilə',\n",
       " 'ilk',\n",
       " 'in',\n",
       " 'indi',\n",
       " 'isə',\n",
       " 'istifadə',\n",
       " 'iyirmi',\n",
       " 'ki',\n",
       " 'kim',\n",
       " 'kimə',\n",
       " 'kimi',\n",
       " 'lakin',\n",
       " 'lap',\n",
       " 'məhz',\n",
       " 'mən',\n",
       " 'mənə',\n",
       " 'mirşey',\n",
       " 'nə',\n",
       " 'nəhayət',\n",
       " 'niyə',\n",
       " 'o',\n",
       " 'obirisi',\n",
       " 'of',\n",
       " 'olan',\n",
       " 'olar',\n",
       " 'olaraq',\n",
       " 'oldu',\n",
       " 'olduğu',\n",
       " 'olmadı',\n",
       " 'olmaz',\n",
       " 'olmuşdur',\n",
       " 'olsun',\n",
       " 'olur',\n",
       " 'on',\n",
       " 'ona',\n",
       " 'ondan',\n",
       " 'onlar',\n",
       " 'onlardan',\n",
       " 'onların ',\n",
       " 'onsuzda',\n",
       " 'onu',\n",
       " 'onun',\n",
       " 'oradan',\n",
       " 'otuz',\n",
       " 'öz',\n",
       " 'özü',\n",
       " 'qarşı',\n",
       " 'qədər',\n",
       " 'qırx',\n",
       " 'saat',\n",
       " 'sadəcə',\n",
       " 'saniyə',\n",
       " 'səhv',\n",
       " 'səkkiz',\n",
       " 'səksən',\n",
       " 'sən',\n",
       " 'sənə',\n",
       " 'sənin',\n",
       " 'siz',\n",
       " 'sizin',\n",
       " 'sizlər',\n",
       " 'sonra',\n",
       " 'təəssüf',\n",
       " 'ü',\n",
       " 'üç',\n",
       " 'üçün',\n",
       " 'var',\n",
       " 'və',\n",
       " 'xan',\n",
       " 'xanım',\n",
       " 'xeyr',\n",
       " 'ya',\n",
       " 'yalnız',\n",
       " 'yaxşı',\n",
       " 'yeddi',\n",
       " 'yenə',\n",
       " 'yəni',\n",
       " 'yetmiş',\n",
       " 'yox',\n",
       " 'yoxdur',\n",
       " 'yoxsa',\n",
       " 'yüz',\n",
       " 'zamanahala',\n",
       " 'aitzitik',\n",
       " 'al',\n",
       " 'ala ',\n",
       " 'alabadere',\n",
       " 'alabaina',\n",
       " 'alabaina',\n",
       " 'aldiz ',\n",
       " 'alta',\n",
       " 'amaitu',\n",
       " 'amaitzeko',\n",
       " 'anitz',\n",
       " 'antzina',\n",
       " 'arabera',\n",
       " 'arabera',\n",
       " 'arabera',\n",
       " 'argi',\n",
       " 'arratsaldero',\n",
       " 'arte',\n",
       " 'artean',\n",
       " 'asko',\n",
       " 'aspaldiko',\n",
       " 'aurrera',\n",
       " 'aurrera',\n",
       " 'azkenez',\n",
       " 'azkenik',\n",
       " 'azkenik',\n",
       " 'ba',\n",
       " 'bada',\n",
       " 'bada ',\n",
       " 'bada ',\n",
       " 'bada ',\n",
       " 'badarik',\n",
       " 'badarik',\n",
       " 'badarik ',\n",
       " 'badere',\n",
       " 'bai',\n",
       " 'baina',\n",
       " 'baina',\n",
       " 'baina ',\n",
       " 'baino',\n",
       " 'baino',\n",
       " 'baino',\n",
       " 'baino',\n",
       " 'baita',\n",
       " 'baizik ',\n",
       " 'baldin',\n",
       " 'baldin',\n",
       " 'barren',\n",
       " 'bat',\n",
       " 'batean',\n",
       " 'batean',\n",
       " 'batean',\n",
       " 'batean',\n",
       " 'batek',\n",
       " 'baten',\n",
       " 'batera',\n",
       " 'batez',\n",
       " 'bati',\n",
       " 'batzuei',\n",
       " 'batzuek',\n",
       " 'batzuetan',\n",
       " 'batzuk',\n",
       " 'bazen',\n",
       " 'bederen',\n",
       " 'bederik',\n",
       " 'beharrez',\n",
       " 'behiala',\n",
       " 'behin',\n",
       " 'behin',\n",
       " 'behin',\n",
       " 'behin',\n",
       " 'behinik',\n",
       " 'behinola',\n",
       " 'behintzat',\n",
       " 'bera',\n",
       " 'beraiek',\n",
       " 'beranduago',\n",
       " 'berau',\n",
       " 'berauek',\n",
       " 'beraz',\n",
       " 'beraz ',\n",
       " 'bere',\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed32a307",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:04.144425Z",
     "start_time": "2023-03-25T08:57:04.141211Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3899dd36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:04.150041Z",
     "start_time": "2023-03-25T08:57:04.145766Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fa17a1a3",
   "metadata": {},
   "source": [
    "Homework:\n",
    "\n",
    "Take motivation from removal of punctuation code, and remove all the stopwords from the sentence and create a new col as we are always doing it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1461c20a",
   "metadata": {},
   "source": [
    "# Happy Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5edd763",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:04.157449Z",
     "start_time": "2023-03-25T08:57:04.151173Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>clean</th>\n",
       "      <th>clean_lower</th>\n",
       "      <th>clean_lower_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "      <td>Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat</td>\n",
       "      <td>go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, cine, there, got, amore, wat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to, 87121, to, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, then, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>nah i dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                                                                            v2  \\\n",
       "0                                              Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...   \n",
       "1                                                                                                                                Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's   \n",
       "3                                                                                                            U dun say so early hor... U c already then say...   \n",
       "4                                                                                                Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "                                                                                                                                                   clean  \\\n",
       "0                                                 Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat   \n",
       "1                                                                                                                                Ok lar Joking wif u oni   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s   \n",
       "3                                                                                                            U dun say so early hor U c already then say   \n",
       "4                                                                                            Nah I dont think he goes to usf he lives around here though   \n",
       "\n",
       "                                                                                                                                             clean_lower  \\\n",
       "0                                                 go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat   \n",
       "1                                                                                                                                ok lar joking wif u oni   \n",
       "2  free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s   \n",
       "3                                                                                                            u dun say so early hor u c already then say   \n",
       "4                                                                                            nah i dont think he goes to usf he lives around here though   \n",
       "\n",
       "                                                                                                                                                                clean_lower_tokenized  \n",
       "0                                                         [go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, cine, there, got, amore, wat]  \n",
       "1                                                                                                                                                      [ok, lar, joking, wif, u, oni]  \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to, 87121, to, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]  \n",
       "3                                                                                                                             [u, dun, say, so, early, hor, u, c, already, then, say]  \n",
       "4                                                                                                           [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "853acbd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:04.160471Z",
     "start_time": "2023-03-25T08:57:04.158465Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    output = [ i for i in text if i not in stopwords.words(\"english\")]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "785bc934",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:10.079668Z",
     "start_time": "2023-03-25T08:57:04.161608Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>clean</th>\n",
       "      <th>clean_lower</th>\n",
       "      <th>clean_lower_tokenized</th>\n",
       "      <th>clean_lower_tokenized_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "      <td>Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat</td>\n",
       "      <td>go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, cine, there, got, amore, wat]</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to, 87121, to, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, then, say]</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>nah i dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                                                                            v2  \\\n",
       "0                                              Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...   \n",
       "1                                                                                                                                Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's   \n",
       "3                                                                                                            U dun say so early hor... U c already then say...   \n",
       "4                                                                                                Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "                                                                                                                                                   clean  \\\n",
       "0                                                 Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat   \n",
       "1                                                                                                                                Ok lar Joking wif u oni   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s   \n",
       "3                                                                                                            U dun say so early hor U c already then say   \n",
       "4                                                                                            Nah I dont think he goes to usf he lives around here though   \n",
       "\n",
       "                                                                                                                                             clean_lower  \\\n",
       "0                                                 go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat   \n",
       "1                                                                                                                                ok lar joking wif u oni   \n",
       "2  free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s   \n",
       "3                                                                                                            u dun say so early hor u c already then say   \n",
       "4                                                                                            nah i dont think he goes to usf he lives around here though   \n",
       "\n",
       "                                                                                                                                                                clean_lower_tokenized  \\\n",
       "0                                                         [go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, cine, there, got, amore, wat]   \n",
       "1                                                                                                                                                      [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to, 87121, to, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]   \n",
       "3                                                                                                                             [u, dun, say, so, early, hor, u, c, already, then, say]   \n",
       "4                                                                                                           [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]   \n",
       "\n",
       "                                                                                                                                   clean_lower_tokenized_stopwords  \n",
       "0                                                              [go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]  \n",
       "1                                                                                                                                   [ok, lar, joking, wif, u, oni]  \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]  \n",
       "3                                                                                                                    [u, dun, say, early, hor, u, c, already, say]  \n",
       "4                                                                                                             [nah, dont, think, goes, usf, lives, around, though]  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_lower_tokenized_stopwords'] = df['clean_lower_tokenized'].apply(lambda x : remove_stopwords(x))\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1bea1985",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "Getting a word to its root form.\n",
    "\n",
    "play, played, playing, playful, playable -> play\n",
    "\n",
    "programmer, programming, program -> program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c41e6561",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:10.082500Z",
     "start_time": "2023-03-25T08:57:10.080647Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3b4d748",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:10.084743Z",
     "start_time": "2023-03-25T08:57:10.083352Z"
    }
   },
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5c2483a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:10.087454Z",
     "start_time": "2023-03-25T08:57:10.085685Z"
    }
   },
   "outputs": [],
   "source": [
    "def stemming(text):\n",
    "    stem_text = [ ps.stem(word) for word in text ]\n",
    "    return stem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a190edf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:10.764034Z",
     "start_time": "2023-03-25T08:57:10.088450Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>clean</th>\n",
       "      <th>clean_lower</th>\n",
       "      <th>clean_lower_tokenized</th>\n",
       "      <th>clean_lower_tokenized_stopwords</th>\n",
       "      <th>clean_lower_tokenized_stopwords_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "      <td>Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat</td>\n",
       "      <td>go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, cine, there, got, amore, wat]</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]</td>\n",
       "      <td>[go, jurong, point, crazi, avail, bugi, n, great, world, la, e, buffet, cine, got, amor, wat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joke, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to, 87121, to, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv, entri, questionstd, txt, ratetc, appli, 08452810075over18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, then, say]</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "      <td>[u, dun, say, earli, hor, u, c, alreadi, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>nah i dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "      <td>[nah, dont, think, goe, usf, live, around, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                                                                            v2  \\\n",
       "0                                              Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...   \n",
       "1                                                                                                                                Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's   \n",
       "3                                                                                                            U dun say so early hor... U c already then say...   \n",
       "4                                                                                                Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "                                                                                                                                                   clean  \\\n",
       "0                                                 Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat   \n",
       "1                                                                                                                                Ok lar Joking wif u oni   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s   \n",
       "3                                                                                                            U dun say so early hor U c already then say   \n",
       "4                                                                                            Nah I dont think he goes to usf he lives around here though   \n",
       "\n",
       "                                                                                                                                             clean_lower  \\\n",
       "0                                                 go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat   \n",
       "1                                                                                                                                ok lar joking wif u oni   \n",
       "2  free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s   \n",
       "3                                                                                                            u dun say so early hor u c already then say   \n",
       "4                                                                                            nah i dont think he goes to usf he lives around here though   \n",
       "\n",
       "                                                                                                                                                                clean_lower_tokenized  \\\n",
       "0                                                         [go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, cine, there, got, amore, wat]   \n",
       "1                                                                                                                                                      [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to, 87121, to, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]   \n",
       "3                                                                                                                             [u, dun, say, so, early, hor, u, c, already, then, say]   \n",
       "4                                                                                                           [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]   \n",
       "\n",
       "                                                                                                                                   clean_lower_tokenized_stopwords  \\\n",
       "0                                                              [go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]   \n",
       "1                                                                                                                                   [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]   \n",
       "3                                                                                                                    [u, dun, say, early, hor, u, c, already, say]   \n",
       "4                                                                                                             [nah, dont, think, goes, usf, lives, around, though]   \n",
       "\n",
       "                                                                                                                          clean_lower_tokenized_stopwords_stem  \n",
       "0                                                                [go, jurong, point, crazi, avail, bugi, n, great, world, la, e, buffet, cine, got, amor, wat]  \n",
       "1                                                                                                                                 [ok, lar, joke, wif, u, oni]  \n",
       "2  [free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv, entri, questionstd, txt, ratetc, appli, 08452810075over18]  \n",
       "3                                                                                                                [u, dun, say, earli, hor, u, c, alreadi, say]  \n",
       "4                                                                                                           [nah, dont, think, goe, usf, live, around, though]  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_lower_tokenized_stopwords_stem'] = df['clean_lower_tokenized_stopwords'].apply(lambda x : stemming(x))\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1aeeaa6",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "Getting a word to its root form plus preserves the lemma i.e. meaning.\n",
    "\n",
    "leaves - Stemming - leav\n",
    "\n",
    "leaves - Lemmatization - leaf\n",
    "\n",
    "goose  - Stemming - goos\n",
    "\n",
    "goose - Lemmatization - goose\n",
    "\n",
    "geese - Stemming - gees\n",
    "\n",
    "geese - Lemmatization - goose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "685984eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:10.766905Z",
     "start_time": "2023-03-25T08:57:10.765028Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a970cbd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:10.769676Z",
     "start_time": "2023-03-25T08:57:10.767736Z"
    }
   },
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    lem_text = [ wnl.lemmatize(word) for word in text]\n",
    "    return lem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c11d829e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:10.772373Z",
     "start_time": "2023-03-25T08:57:10.770766Z"
    }
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7c9073aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:57:11.783761Z",
     "start_time": "2023-03-25T08:57:10.781003Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>clean</th>\n",
       "      <th>clean_lower</th>\n",
       "      <th>clean_lower_tokenized</th>\n",
       "      <th>clean_lower_tokenized_stopwords</th>\n",
       "      <th>clean_lower_tokenized_stopwords_stem</th>\n",
       "      <th>clean_lower_tokenized_stopwords_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "      <td>Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat</td>\n",
       "      <td>go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, cine, there, got, amore, wat]</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]</td>\n",
       "      <td>[go, jurong, point, crazi, avail, bugi, n, great, world, la, e, buffet, cine, got, amor, wat]</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joke, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to, 87121, to, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv, entri, questionstd, txt, ratetc, appli, 08452810075over18]</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, then, say]</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "      <td>[u, dun, say, earli, hor, u, c, alreadi, say]</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>nah i dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "      <td>[nah, dont, think, goe, usf, live, around, though]</td>\n",
       "      <td>[nah, dont, think, go, usf, life, around, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                                                                            v2  \\\n",
       "0                                              Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...   \n",
       "1                                                                                                                                Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's   \n",
       "3                                                                                                            U dun say so early hor... U c already then say...   \n",
       "4                                                                                                Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "                                                                                                                                                   clean  \\\n",
       "0                                                 Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat   \n",
       "1                                                                                                                                Ok lar Joking wif u oni   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive entry questionstd txt rateTCs apply 08452810075over18s   \n",
       "3                                                                                                            U dun say so early hor U c already then say   \n",
       "4                                                                                            Nah I dont think he goes to usf he lives around here though   \n",
       "\n",
       "                                                                                                                                             clean_lower  \\\n",
       "0                                                 go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat   \n",
       "1                                                                                                                                ok lar joking wif u oni   \n",
       "2  free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s   \n",
       "3                                                                                                            u dun say so early hor u c already then say   \n",
       "4                                                                                            nah i dont think he goes to usf he lives around here though   \n",
       "\n",
       "                                                                                                                                                                clean_lower_tokenized  \\\n",
       "0                                                         [go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, cine, there, got, amore, wat]   \n",
       "1                                                                                                                                                      [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to, 87121, to, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]   \n",
       "3                                                                                                                             [u, dun, say, so, early, hor, u, c, already, then, say]   \n",
       "4                                                                                                           [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]   \n",
       "\n",
       "                                                                                                                                   clean_lower_tokenized_stopwords  \\\n",
       "0                                                              [go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]   \n",
       "1                                                                                                                                   [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]   \n",
       "3                                                                                                                    [u, dun, say, early, hor, u, c, already, say]   \n",
       "4                                                                                                             [nah, dont, think, goes, usf, lives, around, though]   \n",
       "\n",
       "                                                                                                                          clean_lower_tokenized_stopwords_stem  \\\n",
       "0                                                                [go, jurong, point, crazi, avail, bugi, n, great, world, la, e, buffet, cine, got, amor, wat]   \n",
       "1                                                                                                                                 [ok, lar, joke, wif, u, oni]   \n",
       "2  [free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv, entri, questionstd, txt, ratetc, appli, 08452810075over18]   \n",
       "3                                                                                                                [u, dun, say, earli, hor, u, c, alreadi, say]   \n",
       "4                                                                                                           [nah, dont, think, goe, usf, live, around, though]   \n",
       "\n",
       "                                                                                                                             clean_lower_tokenized_stopwords_lemma  \n",
       "0                                                              [go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]  \n",
       "1                                                                                                                                   [ok, lar, joking, wif, u, oni]  \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receive, entry, questionstd, txt, ratetcs, apply, 08452810075over18s]  \n",
       "3                                                                                                                    [u, dun, say, early, hor, u, c, already, say]  \n",
       "4                                                                                                                [nah, dont, think, go, usf, life, around, though]  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_lower_tokenized_stopwords_lemma'] = df['clean_lower_tokenized_stopwords'].apply(lambda x : lemmatize(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8eb68868",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T08:59:51.519380Z",
     "start_time": "2023-03-25T08:59:51.513590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ['ok', 'lar', 'joking', 'wif', 'u', 'oni']\n",
      "\n",
      "Stemmed: ['ok', 'lar', 'joke', 'wif', 'u', 'oni']\n",
      "\n",
      "Lemmatized: ['ok', 'lar', 'joking', 'wif', 'u', 'oni']\n"
     ]
    }
   ],
   "source": [
    "print(\"Original:\",df.iloc[1,5])\n",
    "print()\n",
    "print(\"Stemmed:\",df.iloc[1,6])\n",
    "print()\n",
    "print(\"Lemmatized:\",df.iloc[1,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fb2fef20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T09:00:46.945707Z",
     "start_time": "2023-03-25T09:00:46.938061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ['im', 'still', 'looking', 'car', 'buy', 'gone', '4the', 'driving', 'test', 'yet']\n",
      "\n",
      "Stemmed: ['im', 'still', 'look', 'car', 'buy', 'gone', '4the', 'drive', 'test', 'yet']\n",
      "\n",
      "Lemmatized: ['im', 'still', 'looking', 'car', 'buy', 'gone', '4the', 'driving', 'test', 'yet']\n"
     ]
    }
   ],
   "source": [
    "print(\"Original:\",df.iloc[101,5])\n",
    "print()\n",
    "print(\"Stemmed:\",df.iloc[101,6])\n",
    "print()\n",
    "print(\"Lemmatized:\",df.iloc[101,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef07b2aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T09:01:48.010669Z",
     "start_time": "2023-03-25T09:01:48.004423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ['thinked', 'first', 'time', 'saw', 'class']\n",
      "\n",
      "Stemmed: ['think', 'first', 'time', 'saw', 'class']\n",
      "\n",
      "Lemmatized: ['thinked', 'first', 'time', 'saw', 'class']\n"
     ]
    }
   ],
   "source": [
    "print(\"Original:\",df.iloc[50,5])\n",
    "print()\n",
    "print(\"Stemmed:\",df.iloc[50,6])\n",
    "print()\n",
    "print(\"Lemmatized:\",df.iloc[50,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "afb504d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T09:02:42.761233Z",
     "start_time": "2023-03-25T09:02:42.758746Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ['u', 'sending', 'home', 'first', 'ok', 'lor', 'im', 'ready', 'yet']\n",
      "\n",
      "Stemmed: ['u', 'send', 'home', 'first', 'ok', 'lor', 'im', 'readi', 'yet']\n",
      "\n",
      "Lemmatized: ['u', 'sending', 'home', 'first', 'ok', 'lor', 'im', 'ready', 'yet']\n"
     ]
    }
   ],
   "source": [
    "print(\"Original:\",df.iloc[601,5])\n",
    "print()\n",
    "print(\"Stemmed:\",df.iloc[601,6])\n",
    "print()\n",
    "print(\"Lemmatized:\",df.iloc[601,7])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c57f681a",
   "metadata": {},
   "source": [
    "# Stemming Techniques\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2021/11/an-introduction-to-stemming-in-natural-language-processing/#:~:text=Stemming%20is%20a%20natural%20language,and%20documents%20for%20text%20normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b34f9f32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T09:05:33.643269Z",
     "start_time": "2023-03-25T09:05:33.629971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connects ---> connect\n",
      "Connecting ---> connect\n",
      "Connections ---> connect\n",
      "Connected ---> connect\n",
      "Connection ---> connect\n",
      "Connectings ---> connect\n",
      "Connect ---> connect\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "words = ['Connects','Connecting','Connections','Connected',\n",
    "         'Connection','Connectings','Connect']\n",
    "\n",
    "for word in words:\n",
    "    print(word,\"--->\",porter.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ae3bf172",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T09:06:41.552490Z",
     "start_time": "2023-03-25T09:06:41.546243Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous ---> generous\n",
      "generate ---> generat\n",
      "generously ---> generous\n",
      "generation ---> generat\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowball = SnowballStemmer(language='english')\n",
    "words = ['generous','generate','generously','generation']\n",
    "\n",
    "for word in words:\n",
    "    print(word,\"--->\",snowball.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "19195a01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T09:07:33.479761Z",
     "start_time": "2023-03-25T09:07:33.471127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating ---> eat\n",
      "eats ---> eat\n",
      "eaten ---> eat\n",
      "puts ---> put\n",
      "putting ---> put\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lancaster = LancasterStemmer()\n",
    "words = ['eating','eats','eaten','puts','putting']\n",
    "for word in words:\n",
    "    print(word,\"--->\",lancaster.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3c641777",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T09:08:55.391253Z",
     "start_time": "2023-03-25T09:08:55.383961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mass ---> mas\n",
      "was ---> was\n",
      "bee ---> bee\n",
      "computer ---> computer\n",
      "advisable ---> advis\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "regexp = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
    "words = ['mass','was','bee','computer','advisable']\n",
    "for word in words:\n",
    "    print(word,\"--->\",regexp.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b3073fda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T09:09:56.900572Z",
     "start_time": "2023-03-25T09:09:56.890671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Porter Stemmer      Snowball Stemmer    Lancaster Stemmer             Regexp Stemmer                          \n",
      "friend              friend              friend              friend                        friend                                  \n",
      "friendship          friendship          friendship          friend                        friendship                              \n",
      "friends             friend              friend              friend                        friend                                  \n",
      "friendships         friendship          friendship          friend                        friendship                              \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, RegexpStemmer\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer(language='english')\n",
    "regexp = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\"]\n",
    "print(\"{0:20}{1:20}{2:20}{3:30}{4:40}\".format(\"Word\",\"Porter Stemmer\",\"Snowball Stemmer\",\"Lancaster Stemmer\",'Regexp Stemmer'))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}{3:30}{4:40}\".format(word,porter.stem(word),snowball.stem(word),lancaster.stem(word),regexp.stem(word)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9b5497f",
   "metadata": {},
   "source": [
    "# Lemmatization Techniques\n",
    "\n",
    "https://www.geeksforgeeks.org/python-lemmatization-with-nltk/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fb49c9e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T09:17:26.016805Z",
     "start_time": "2023-03-25T09:17:26.009074Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : good\n",
      "better : better\n"
     ]
    }
   ],
   "source": [
    "# import these modules\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
    "\n",
    "# a denotes adjective in \"pos\"\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7825666f",
   "metadata": {},
   "source": [
    "https://www.machinelearningplus.com/nlp/lemmatization-examples-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6d1588ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T09:20:10.286302Z",
     "start_time": "2023-03-25T09:20:10.278574Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strip\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"stripes\", 'v')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ab5834b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T09:20:37.921204Z",
     "start_time": "2023-03-25T09:20:37.919178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stripe\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"stripes\", 'n'))  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c423956",
   "metadata": {},
   "source": [
    "Install spacy\n",
    "\n",
    "https://spacy.io/usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "97043443",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T09:25:56.945059Z",
     "start_time": "2023-03-25T09:25:56.530201Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5b950e30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T09:27:54.434507Z",
     "start_time": "2023-03-25T09:27:54.065950Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the stripe bat be hang on their foot for good\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "\n",
    "# Parse the sentence using the loaded 'en' model object `nlp`\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# # Extract the lemma for each token and join\n",
    "print(\" \".join([token.lemma_ for token in doc]))\n",
    "# #> 'the strip bat be hang on -PRON- foot for good'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb44d69c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T09:30:35.047952Z",
     "start_time": "2023-03-25T09:30:31.889141Z"
    }
   },
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "87c3e13c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T09:31:02.004914Z",
     "start_time": "2023-03-25T09:31:01.982777Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stripe'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob, Word\n",
    "\n",
    "# Lemmatize a word\n",
    "word = 'stripes'\n",
    "w = Word(word)\n",
    "w.lemmatize()\n",
    "#> stripe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4ce262aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T09:31:29.425638Z",
     "start_time": "2023-03-25T09:31:29.415982Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The striped bat are hanging on their foot for best'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatize a sentence\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "sent = TextBlob(sentence)\n",
    "\" \". join([w.lemmatize() for w in sent.words])\n",
    "#> 'The striped bat are hanging on their foot for best'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce3b15a4",
   "metadata": {},
   "source": [
    "# TextBlob\n",
    "\n",
    "https://textblob.readthedocs.io/en/dev/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f99db8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:14:56.009464Z",
     "start_time": "2023-03-25T10:14:54.089701Z"
    }
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3be4dde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:15:21.583459Z",
     "start_time": "2023-03-25T10:15:21.577865Z"
    }
   },
   "outputs": [],
   "source": [
    "wiki = TextBlob(\"Python is a high-level, general-purpose programming language.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fab9c1ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:15:24.470759Z",
     "start_time": "2023-03-25T10:15:24.448459Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Python is a high-level, general-purpose programming language.\")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "101c7e20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:15:38.015131Z",
     "start_time": "2023-03-25T10:15:38.009200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('high-level', 'JJ'),\n",
       " ('general-purpose', 'JJ'),\n",
       " ('programming', 'NN'),\n",
       " ('language', 'NN')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.tags\n",
    "# https://www.ibm.com/docs/en/wca/3.5.0?topic=analytics-part-speech-tag-sets"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b57eb516",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:18:02.605260Z",
     "start_time": "2023-03-25T10:18:01.090336Z"
    }
   },
   "source": [
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a19bfc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:18:07.738647Z",
     "start_time": "2023-03-25T10:18:05.935842Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['python'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9ddc284",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:18:49.696860Z",
     "start_time": "2023-03-25T10:18:49.599920Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.39166666666666666, subjectivity=0.4357142857142857)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testimonial = TextBlob(\"Textblob is amazingly simple to use. What great fun!\")\n",
    "testimonial.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ec27dc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:20:08.469317Z",
     "start_time": "2023-03-25T10:20:08.461042Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39166666666666666"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testimonial.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "168aaf58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:20:17.826528Z",
     "start_time": "2023-03-25T10:20:17.821096Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4357142857142857"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testimonial.sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df13f6b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:22:19.838116Z",
     "start_time": "2023-03-25T10:22:19.834425Z"
    }
   },
   "outputs": [],
   "source": [
    "zen = TextBlob(\"Beautiful is better than ugly. \"\n",
    "...                \"Explicit is better than implicit. \"\n",
    "...                \"Simple is better than complex.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9573a831",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:22:20.604930Z",
     "start_time": "2023-03-25T10:22:20.596787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex.\")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68b40b2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:22:21.319945Z",
     "start_time": "2023-03-25T10:22:21.313246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Beautiful', 'is', 'better', 'than', 'ugly', 'Explicit', 'is', 'better', 'than', 'implicit', 'Simple', 'is', 'better', 'than', 'complex'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zen.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7265d491",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:22:34.567179Z",
     "start_time": "2023-03-25T10:22:34.559238Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"Beautiful is better than ugly.\"),\n",
       " Sentence(\"Explicit is better than implicit.\"),\n",
       " Sentence(\"Simple is better than complex.\")]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zen.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37b93db1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:22:54.224778Z",
     "start_time": "2023-03-25T10:22:54.217166Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.2166666666666667, subjectivity=0.8333333333333334)\n",
      "Sentiment(polarity=0.5, subjectivity=0.5)\n",
      "Sentiment(polarity=0.06666666666666667, subjectivity=0.41904761904761906)\n"
     ]
    }
   ],
   "source": [
    "for sentence in zen.sentences:\n",
    "...     print(sentence.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ee60772",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:24:39.582316Z",
     "start_time": "2023-03-25T10:24:39.575160Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Use', '4', 'spaces', 'per', 'indentation', 'level'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = TextBlob('Use 4 spaces per indentation level.')\n",
    "sentence.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "810d24eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:24:41.135319Z",
     "start_time": "2023-03-25T10:24:41.125724Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'space'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.words[2].singularize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "280dd7c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:24:41.421136Z",
     "start_time": "2023-03-25T10:24:41.412182Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'levels'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.words[-1].pluralize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19fb51d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:25:28.530222Z",
     "start_time": "2023-03-25T10:25:27.522673Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'octopus'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "w = Word(\"octopi\")\n",
    "w.lemmatize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "608e5f78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:25:57.379754Z",
     "start_time": "2023-03-25T10:25:57.373936Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = Word(\"went\")\n",
    "w.lemmatize(\"v\")  # Pass in WordNet part of speech (verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef664a77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:27:33.434110Z",
     "start_time": "2023-03-25T10:27:33.347265Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('octopus.n.01'), Synset('octopus.n.02')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "from textblob.wordnet import VERB\n",
    "word = Word(\"octopus\")\n",
    "word.synsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5053ef48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:28:04.573466Z",
     "start_time": "2023-03-25T10:28:04.542604Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('chop.v.05'),\n",
       " Synset('hack.v.02'),\n",
       " Synset('hack.v.03'),\n",
       " Synset('hack.v.04'),\n",
       " Synset('hack.v.05'),\n",
       " Synset('hack.v.06'),\n",
       " Synset('hack.v.07'),\n",
       " Synset('hack.v.08')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word(\"hack\").get_synsets(pos=VERB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2c39574",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:28:51.887506Z",
     "start_time": "2023-03-25T10:28:51.878085Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tentacles of octopus prepared as food',\n",
       " 'bottom-living cephalopod having a soft oval body with eight long tentacles']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word(\"octopus\").definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7d389e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:29:11.499833Z",
     "start_time": "2023-03-25T10:29:11.486365Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a fierce or audacious person',\n",
       " 'large feline of forests in most of Asia having a tawny coat with black stripes; endangered']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word(\"tiger\").definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e397645",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:29:36.007931Z",
     "start_time": "2023-03-25T10:29:36.000117Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word(\"chatgpt\").definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2108b278",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:32:28.605196Z",
     "start_time": "2023-03-25T10:32:28.597127Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a portable computer small enough to use in your lap']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word(\"laptop\").definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef3c0add",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:33:35.105465Z",
     "start_time": "2023-03-25T10:33:35.095080Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a flexible container with a single opening',\n",
       " 'the quantity of game taken in a particular period (usually by one person)',\n",
       " 'a place that the runner must touch before scoring',\n",
       " 'a container used for carrying money and small personal items or accessories (especially by women)',\n",
       " 'the quantity that a bag will hold',\n",
       " 'a portable rectangular container for carrying clothes',\n",
       " 'an ugly or ill-tempered woman',\n",
       " 'mammary gland of bovids (cows and sheep and goats)',\n",
       " 'an activity that you like or at which you are superior',\n",
       " 'capture or kill, as in hunting',\n",
       " 'hang loosely, like an empty bag',\n",
       " 'bulge out; form a bulge outward, or be so full as to appear to bulge',\n",
       " 'take unlawfully',\n",
       " 'put into a bag']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word(\"bag\").definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "972cd18b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:33:42.102367Z",
     "start_time": "2023-03-25T10:33:42.092515Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an artifact designed to be played with',\n",
       " 'a nonfunctional replica of something else (frequently used as a modifier)',\n",
       " 'a device regarded as providing amusement',\n",
       " 'a copy that reproduces a person or thing in greatly reduced size',\n",
       " 'any of several breeds of very small dogs kept purely as pets',\n",
       " 'behave carelessly or indifferently',\n",
       " \"manipulate manually or in one's mind or imagination\",\n",
       " 'engage in an activity as if it were a game rather than take it seriously']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word(\"toy\").definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "07443ef0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:33:53.253650Z",
     "start_time": "2023-03-25T10:33:53.245585Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(ethnic slur) offensive and derogatory name for a Black man who is abjectly servile and deferential to Whites',\n",
       " 'male cat',\n",
       " 'male turkey']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word(\"tom\").definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "54bca2a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:34:03.056468Z",
     "start_time": "2023-03-25T10:34:03.050483Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['offensive term for a person of German descent']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word(\"jerry\").definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e7794a36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:36:32.345515Z",
     "start_time": "2023-03-25T10:36:32.330509Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1111111111111111"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob.wordnet import Synset\n",
    "octopus = Synset('octopus.n.02')\n",
    "shrimp = Synset('shrimp.n.03')\n",
    "octopus.path_similarity(shrimp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "25749372",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:37:11.639574Z",
     "start_time": "2023-03-25T10:37:11.631717Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['cat', 'dog', 'octopus'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animals = TextBlob(\"cat dog octopus\")\n",
    "animals.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "82559d79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:37:55.037004Z",
     "start_time": "2023-03-25T10:37:55.030854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['cats', 'dogs', 'octopodes'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animals.words.pluralize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0e39c1af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:38:35.112630Z",
     "start_time": "2023-03-25T10:38:35.056001Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have good spelling!\n"
     ]
    }
   ],
   "source": [
    "b = TextBlob(\"I havv goood speling!\")\n",
    "print(b.correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d504ab19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:38:59.456864Z",
     "start_time": "2023-03-25T10:38:59.264482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a good knowledge of spelling and computer!\n"
     ]
    }
   ],
   "source": [
    "b = TextBlob(\"I havv a goood knowladge of speling and campoter!\")\n",
    "print(b.correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c7e5eae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:40:12.349804Z",
     "start_time": "2023-03-25T10:40:12.340159Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fallibility', 1.0)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "w = Word('falibility')\n",
    "w.spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "64a39f27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:41:03.717805Z",
     "start_time": "2023-03-25T10:41:03.535040Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('computer', 0.75), ('composer', 0.1875), ('capotes', 0.0625)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "w = Word('campoter')\n",
    "w.spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "789affdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:41:39.877442Z",
     "start_time": "2023-03-25T10:41:39.753008Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('leptop', 0.0)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "w = Word('leptop')\n",
    "w.spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4704966c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:41:55.027557Z",
     "start_time": "2023-03-25T10:41:54.924896Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('last', 0.6524249422632794),\n",
       " ('late', 0.1905311778290993),\n",
       " ('lamp', 0.042725173210161664),\n",
       " ('apt', 0.04041570438799076),\n",
       " ('klapp', 0.016166281755196306),\n",
       " ('lap', 0.012702078521939953),\n",
       " ('rapp', 0.011547344110854504),\n",
       " ('lasts', 0.011547344110854504),\n",
       " ('lapse', 0.009237875288683603),\n",
       " ('lata', 0.004618937644341801),\n",
       " ('rapt', 0.003464203233256351),\n",
       " ('leapt', 0.0023094688221709007),\n",
       " ('laity', 0.0023094688221709007)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "w = Word('laptp')\n",
    "w.spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "478694c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:42:11.036320Z",
     "start_time": "2023-03-25T10:42:10.884083Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('laptopp', 0.0)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "w = Word('laptopp')\n",
    "w.spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "217b1ed5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:42:18.754318Z",
     "start_time": "2023-03-25T10:42:18.630118Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lawton', 1.0)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "w = Word('laptop')\n",
    "w.spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f89572d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:42:43.858096Z",
     "start_time": "2023-03-25T10:42:43.849489Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spectacle', 1.0)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "w = Word('spectace')\n",
    "w.spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "94bb9253",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:43:09.588646Z",
     "start_time": "2023-03-25T10:43:09.579271Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('circuit', 1.0)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "w = Word('cirkuit')\n",
    "w.spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "42d7fd96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:43:18.927123Z",
     "start_time": "2023-03-25T10:43:18.919350Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barber', 0.8), ('barbed', 0.2)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "w = Word('barbeq')\n",
    "w.spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3fddb1f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:43:32.110944Z",
     "start_time": "2023-03-25T10:43:31.958855Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barber', 0.8), ('barbed', 0.2)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "w = Word('barbequ')\n",
    "w.spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3fdd69a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:43:38.287827Z",
     "start_time": "2023-03-25T10:43:38.103538Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barque', 1.0)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "w = Word('barbeque')\n",
    "w.spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4c3867c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:44:12.073607Z",
     "start_time": "2023-03-25T10:44:12.064994Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"We are no longer the Knights who say Ni. We are now the Knights who say Ekki ekki ekki PTANG.\")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty = TextBlob(\"We are no longer the Knights who say Ni. \"\n",
    "...                     \"We are now the Knights who say Ekki ekki ekki PTANG.\")\n",
    "monty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1882cf3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:44:44.468750Z",
     "start_time": "2023-03-25T10:44:44.461830Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty.word_counts['ekki']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3d06833d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:45:04.102927Z",
     "start_time": "2023-03-25T10:45:04.096582Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty.words.count('ekki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e7281e3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:45:14.471917Z",
     "start_time": "2023-03-25T10:45:14.463706Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty.words.count('ekki', case_sensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dded8ee7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:46:36.515703Z",
     "start_time": "2023-03-25T10:46:36.510173Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Python is a high-level, general-purpose programming language.\")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9bd26846",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:46:36.738328Z",
     "start_time": "2023-03-25T10:46:36.735542Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.noun_phrases.count('python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8cbd97a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:52:27.186558Z",
     "start_time": "2023-03-25T10:52:27.056490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And/CC/O/O now/RB/B-ADVP/O for/IN/B-PP/B-PNP something/NN/B-NP/I-PNP completely/RB/B-ADJP/O different/JJ/I-ADJP/O ././O/O\n"
     ]
    }
   ],
   "source": [
    "b = TextBlob(\"And now for something completely different.\")\n",
    "print(b.parse())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "574d29f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:53:03.094610Z",
     "start_time": "2023-03-25T10:53:03.085230Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex.\")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5a655ae2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:53:06.125365Z",
     "start_time": "2023-03-25T10:53:06.116897Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Beautiful is better\")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zen[0:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "00df4ae2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:53:24.443424Z",
     "start_time": "2023-03-25T10:53:24.435573Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"BEAUTIFUL IS BETTER THAN UGLY. EXPLICIT IS BETTER THAN IMPLICIT. SIMPLE IS BETTER THAN COMPLEX.\")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zen.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cb44cc18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:54:10.754829Z",
     "start_time": "2023-03-25T10:54:10.745965Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zen.find(\"Simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ec052bcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:55:27.791880Z",
     "start_time": "2023-03-25T10:55:27.785714Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex.\")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d09243d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:55:35.726607Z",
     "start_time": "2023-03-25T10:55:35.719491Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zen.find(\"Beautiful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c6b36b2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:56:10.855809Z",
     "start_time": "2023-03-25T10:56:10.846158Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple_blob = TextBlob('apples')\n",
    "banana_blob = TextBlob('bananas')\n",
    "apple_blob < banana_blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "549d82c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:56:30.421475Z",
     "start_time": "2023-03-25T10:56:30.413448Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple_blob == 'apples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ed42a601",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:57:02.782034Z",
     "start_time": "2023-03-25T10:57:02.773932Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apples and bananas'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(\"Now is better than never.\")\n",
    ">>> blob.ngrams(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cb62169a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:58:18.717344Z",
     "start_time": "2023-03-25T10:58:18.710505Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['good', 'night']),\n",
       " WordList(['night', 'sweet']),\n",
       " WordList(['sweet', 'dreams']),\n",
       " WordList(['dreams', 'all']),\n",
       " WordList(['all', 'the']),\n",
       " WordList(['the', 'best'])]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(\"good night sweet dreams all the best\")\n",
    "blob.ngrams(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5d5e6f75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:58:23.525267Z",
     "start_time": "2023-03-25T10:58:23.516038Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['good', 'night', 'sweet']),\n",
       " WordList(['night', 'sweet', 'dreams']),\n",
       " WordList(['sweet', 'dreams', 'all']),\n",
       " WordList(['dreams', 'all', 'the']),\n",
       " WordList(['all', 'the', 'best'])]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(\"good night sweet dreams all the best\")\n",
    "blob.ngrams(n=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e2aafbc",
   "metadata": {},
   "source": [
    "## Advanced TextBlob\n",
    "\n",
    "https://textblob.readthedocs.io/en/dev/advanced_usage.html#advanced\n",
    "\n",
    "# Happy Learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
